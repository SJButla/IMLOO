{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEuOtH5ACgZi7vZx9qBoXI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SJButla/IMLOO/blob/main/IMLO_coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for the training set\n",
        "my_train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(192),  # Randomly crop and resize the image to 192x192\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
        "    transforms.RandomRotation(30),  # Randomly rotate the image by up to 30 degrees\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Randomly change brightness, contrast, saturation, and hue\n",
        "    transforms.ToTensor(),  # Convert image to a tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image with mean and std deviation\n",
        "])\n",
        "\n",
        "# Define transformations for the test set\n",
        "my_test_transform = transforms.Compose([\n",
        "    transforms.Resize(224),  # Resize the image to 224x224\n",
        "    transforms.CenterCrop(192),  # Center crop the image to 192x192\n",
        "    transforms.ToTensor(),  # Convert image to a tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image with mean and std deviation\n",
        "])\n",
        "# Load the Flowers102 dataset for training and test sets\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=my_train_transform)\n",
        "test_data = datasets.Flowers102(root='data', split='test', download=True, transform=my_test_transform)\n",
        "batch_size = 64  # Batch size for data loading\n",
        "# Split the training dataset into training and validation sets (80% training, 20% validation)\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_data, val_data = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoader for training, validation, and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n"
      ],
      "metadata": {
        "id": "ZtF4uDxiGrWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the Flowers102 dataset for training and test sets\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=my_train_transform)\n",
        "test_data = datasets.Flowers102(root='data', split='test', download=True, transform=my_test_transform)\n",
        "batch_size = 64  # Batch size for data loading\n",
        "# Split the training dataset into training and validation sets (80% training, 20% validation)\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_data, val_data = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoader for training, validation, and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n"
      ],
      "metadata": {
        "id": "uwKvUQ2lGoTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the neural network model\n",
        "class FlowerClassifier(nn.Module):\n",
        "    def __init__(self, dropout_r=0.5):\n",
        "        super(FlowerClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)  # First convolutional layer\n",
        "        self.bn1 = nn.BatchNorm2d(64)  # Batch normalization for the first layer\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)  # Second convolutional layer\n",
        "        self.bn2 = nn.BatchNorm2d(128)  # Batch normalization for the second layer\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)  # Third convolutional layer\n",
        "        self.bn3 = nn.BatchNorm2d(256)  # Batch normalization for the third layer\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)  # Fourth convolutional layer\n",
        "        self.bn4 = nn.BatchNorm2d(512)  # Batch normalization for the fourth layer\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling layer\n",
        "        self.dropout = nn.Dropout(dropout_r)  # Dropout layer for regularization\n",
        "        self.fc1 = nn.Linear(512 * 12 * 12, 102)  # Fully connected layer for classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # Apply conv1, batch norm, ReLU, and max pooling\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # Apply conv2, batch norm, ReLU, and max pooling\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # Apply conv3, batch norm, ReLU, and max pooling\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))  # Apply conv4, batch norm, ReLU, and max pooling\n",
        "        x = x.view(-1, 512 * 12 * 12)  # Flatten the tensor for the fully connected layer\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.fc1(x)  # Apply the fully connected layer\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "dUP4_GUvGw2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_r = 0.001  # Learning rate for the optimizer\n",
        "weight_decay = 0.05  # Weight decay for regularization\n",
        "dropout_r = 0.5  # Dropout rate for the dropout layer\n",
        "\n",
        "\n",
        "# Set the device for computation (GPU if available, else CPU)\n",
        "my_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = FlowerClassifier(dropout_r=dropout_r).to(my_device)\n",
        "loss_function = nn.CrossEntropyLoss()  # Loss function for classification\n",
        "optimiser = optim.Adam(model.parameters(), lr=learning_r, weight_decay=weight_decay)  # Adam optimizer\n",
        "\n",
        "# Setup the learning rate scheduler\n",
        "step = optim.lr_scheduler.StepLR(optimiser, step_size=30, gamma=0.8)  # StepLR scheduler to reduce learning rate\n"
      ],
      "metadata": {
        "id": "D7C-Q0gGG1Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0  # Initialize the count of correct predictions\n",
        "    total = 0  # Initialize the count of total samples\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for input, label in loader:  # Iterate over the data loader\n",
        "            input = input.to(my_device)  # Move inputs to the device\n",
        "            label = label.to(my_device)  # Move labels to the device\n",
        "            outputs = model(input)  # Get model predictions\n",
        "            _, preds = torch.max(outputs, 1)  # Get the predicted class\n",
        "            correct += torch.sum(preds == label.data)  # Count the correct predictions\n",
        "            total += label.size(0)  # Count the total samples\n",
        "    return (correct.double() / total) * 100  # Return the accuracy as a percentage\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, train_loader, val_loader, test_loader, loss_function, optimizer, scheduler, num_epochs=500):\n",
        "    my_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    best_val_accuracy = 0.0  # Initialize the best validation accuracy\n",
        "    for epoch in range(num_epochs):  # Iterate over the epochs\n",
        "        model.train()  # Set the model to training mode\n",
        "        running_loss = 0.0  # Initialize running loss\n",
        "        correct = 0  # Initialize the count of correct predictions\n",
        "        total = 0  # Initialize the count of total samples\n",
        "        for input, label in train_loader:  # Iterate over the training data loader\n",
        "            input = input.to(my_device)  # Move inputs to the device\n",
        "            label = label.to(my_device)  # Move labels to the device\n",
        "            optimiser.zero_grad()  # Zero the gradients\n",
        "            outputs = model(input)  # Get model predictions\n",
        "            loss = loss_function(outputs, label)  # Compute the loss\n",
        "            loss.backward()  # Backpropagate the loss\n",
        "            optimiser.step()  # Update the model parameters\n",
        "            running_loss += loss.item()  # Accumulate the loss\n",
        "            _, preds = torch.max(outputs, 1)  # Get the predicted class\n",
        "            correct += torch.sum(preds == label.data)  # Count the correct predictions\n",
        "            total += label.size(0)  # Count the total samples\n",
        "\n",
        "        step.step()  # Update the learning rate after each epoch\n",
        "\n",
        "        if (epoch + 1) % 50 == 0:  # Print information every 50 epochs\n",
        "            epoch_loss = running_loss / len(train_loader.dataset)  # Compute the epoch loss\n",
        "            train_accuracy = (correct.double() / total) * 100  # Compute the training accuracy\n",
        "            val_accuracy = calculate_accuracy(val_loader, model)  # Compute the validation accuracy\n",
        "\n",
        "            if val_accuracy > best_val_accuracy:  # Check if validation accuracy improved\n",
        "                best_val_accuracy = val_accuracy  # Update the best validation accuracy\n",
        "                torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Batch Size: {batch_size}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete')\n",
        "    model.load_state_dict(torch.load('best_model.pth'))  # Load the best model\n",
        "    return model"
      ],
      "metadata": {
        "id": "HzL3AhJIG82L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "trained_model = train_model(model, train_loader, val_loader, test_loader, loss_function, optimiser, step, num_epochs=500)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "def evaluate_model(model, test_loader):\n",
        "    test_accuracy = calculate_accuracy(test_loader, model)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "evaluate_model(trained_model, test_loader)"
      ],
      "metadata": {
        "id": "BPp3uoUSHAHh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}